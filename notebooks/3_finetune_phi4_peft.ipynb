{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c5ee60",
   "metadata": {},
   "source": [
    "# Fine-Tuning Phi-4 with PEFT (LoRA)\n",
    "\n",
    "This notebook shows how to fine-tune the `OpenAI/phi-4` model from Hugging Face using the PEFT library with LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3d893",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers datasets accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25b8b2",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f64efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/phi-4\"\n",
    "OUTPUT_DIR = \"./phi4_finetuned\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5547007",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load a sample instruction dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3911562",
   "metadata": {},
   "source": [
    "## 4. Preprocess & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e365ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(examples):\n",
    "    inputs = [\"### Instruction:\\n\" + ins + \"\\n### Response:\\n\" for ins in examples[\"instruction\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_SEQ_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=MAX_SEQ_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized = dataset.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\"]\n",
    ")\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aaf7de",
   "metadata": {},
   "source": [
    "## 5. Prepare Model with LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ab676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model in 8-bit for memory efficiency\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Set up LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf67403",
   "metadata": {},
   "source": [
    "## 6. Data Collator & Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1078a9d",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized.get(\"test\"),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12071b34",
   "metadata": {},
   "source": [
    "## 8. Save Fine-Tuned Model and Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e434c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model and adapters saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a194f",
   "metadata": {},
   "source": [
    "## 9. Inference Example\n",
    "```python\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=OUTPUT_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")\n",
    "print(generator('Tell me more about generative AI', max_length=150, do_sample=True))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roleready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
