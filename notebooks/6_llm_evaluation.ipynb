{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuIK_fkWvIYx"
      },
      "source": [
        "# Why LLM Evals?\n",
        "Traditional software testing isn't applicable on LLM's because their output isn't deterministic. BUt you still need a way to judge the quality of your LLM outputs.\n",
        "\n",
        "When you adjust your prompts or user input's different data, how will you know whether your application has improved and by how much using?\n",
        "\n",
        "Well, say hello to LLM Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqbbHtAtT_2r"
      },
      "source": [
        "Traditionally Machine Learning output was evaluated on certain metrics depending on area of application like NLP, Ranking Alogithms, Regression Tasks etc. Some common metrics are-\n",
        "\n",
        "1.   Accuracy\n",
        "2.   Precision\n",
        "3.   Recall\n",
        "4. Mean Absolute Error (MAE)\n",
        "5. Mean Squared Error (MSE)\n",
        "6. BLEU (Bilingual Evaluation Understudy)\n",
        "7. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "8. Precision@k\n",
        "9. Mean Reciprocal Rank (MRR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWfTAMypVBjf"
      },
      "source": [
        "# Let's see some of them in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbWDgu9Ulsw9"
      },
      "outputs": [],
      "source": [
        "# !pip install evaluate\n",
        "# !pip install nltk\n",
        "# !pip install --upgrade boto3 botocore s3fs aiobotocore\n",
        "# !pip install deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3-YKw1Qo0BE"
      },
      "source": [
        "Let's compare the two strings to see how similar they are, let's use BLEU as a metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP-PD2pXCNuV"
      },
      "source": [
        "Task: We have a story of fox jumping over the dog and let's say the AI system was aksed to summarize it.\n",
        "To evalaute it, we will ask human also to write a summary and we will comapre the two using some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50VG1T2yaVRj",
        "outputId": "d67ab44b-b1fe-4efe-f7d2-43736dcc8eb3"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from pprint import pprint\n",
        "\n",
        "bleu = evaluate.load('bleu')\n",
        "\n",
        "predictions = [\"A quick brown fox jumps over a lazy dog\"]\n",
        "human_response = [\"A quick brown fox jumps over a dog\"]\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=human_response)\n",
        "pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtqVyGl_rUtZ"
      },
      "source": [
        "You can use multiple metrics for the same task and get more indicators of perfromance of your model. Let's also calculate METEOR score for a comparing AI generated summary with a human expected summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U28td6GQlrLC",
        "outputId": "7d562acf-27f0-477e-c639-108e3b3a44f1"
      },
      "outputs": [],
      "source": [
        "\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "predictions = [\"A quick brown fox jumps over a lazy dog\"]\n",
        "human_response = [[\"A quick brown fox jumps over a dog\"]]\n",
        "\n",
        "results = meteor.compute(predictions=predictions, references=human_response)\n",
        "pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh-g0wjRsbnb"
      },
      "source": [
        "## Is there a problem here?\n",
        "\n",
        "1.   Need of labelled data\n",
        "\n",
        "  1.   Labelled data is expensive to annotate, i.e, getting that human_response is hard\n",
        "  2.   It is biased by human judgment and intelligence.\n",
        "  3.   It's not scalable\n",
        "  2.   It's still not accurate enough.\n",
        "\n",
        "2.   Lack of semantic understanding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJwUC76mBK-G"
      },
      "source": [
        "\n",
        "### Another problem look:\n",
        "**Task:**\n",
        "\n",
        "In **question answering task** like this one below:\n",
        "\n",
        "**Context:** Working at mines is not easy. Workers work all day. After a long day at work, the workers go to the their beds to rest for the night. And there is hardly anything else to do.\n",
        "\n",
        "**Question:** What did the workers do after finishing their work?\n",
        "\n",
        "**Human answer:** They went to their beds to sleep and rest for the night.\n",
        "\n",
        "**AI generated answer:** They went to sleep.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWHsZibYXD8m"
      },
      "source": [
        "Let's apply the BLEU on this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJoebe8Rn3aj",
        "outputId": "1eabbf59-4654-4fe7-da08-48eb924aef02"
      },
      "outputs": [],
      "source": [
        "ai_response = [\"They went to their beds to sleep and rest for the night.\"]\n",
        "human_response = [[\"They went to sleep\"]]\n",
        "\n",
        "results = bleu.compute(predictions=ai_response, references=human_response)\n",
        "pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU5UkVW0CBKc"
      },
      "source": [
        "Don't worry, let's use METEOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZCdTsPuYZgX",
        "outputId": "7d929072-f9bf-42cc-d87a-e0e4bd3851e5"
      },
      "outputs": [],
      "source": [
        "ai_response = [\"They went to their beds to sleep and rest for the night.\"]\n",
        "human_response = [[\"Went to rest\"]]\n",
        "\n",
        "results = meteor.compute(predictions=ai_response, references=human_response)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tChWlDimvAoG"
      },
      "source": [
        "## Oh, that didn't work!\n",
        "\n",
        "What's the solution here. Well, LLMs to the rescue âœˆ\n",
        "\n",
        "Let's use LLMs to evaluate LLM outputs .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlq6_ufU1tY_"
      },
      "source": [
        "Python library to the rescue - [deepeval](https://github.com/confident-ai/deepeval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsJ54ltB_FXC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from deepeval.models import GPTModel\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "openai_eval = GPTModel(\n",
        "    model=\"gpt-4.1-mini\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "f64a606acc9b45d3b9cdd35b9ad9b043",
            "15e1b33f415f4b5b81d5e199a02c30d0"
          ]
        },
        "id": "Mq7lnXrY06FH",
        "outputId": "798dd2e5-ef7a-4a51-cdb5-3b5e1ef1465d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=openai_eval)\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What did the workers do after finishing their work?\",\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=\"They went for rest.\",\n",
        "    expected_output=\"They went to their beds to sleep for the night\",\n",
        "    retrieval_context=[\"After a long day at work, the workers went to the their beds to rest for the night\"]\n",
        ")\n",
        "evaluate([test_case], [answer_relevancy_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ0r7GpqBs0v"
      },
      "source": [
        "## Pretty cool, right!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5pU6S6XB261"
      },
      "source": [
        "Let's take it up a notch. What if we didn't have the ground/golden truth/labelled output by human expert?\n",
        "\n",
        "Good news, LLMs have world knowledge, they can act like human for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "3b9caa9bbd9648acad0d7a4b27bc135e",
            "22b9cc92bff84586970d7268ac602163"
          ]
        },
        "id": "l3FCWnkXBzbn",
        "outputId": "7f8f8694-e3c6-443f-c097-2e9ceff980ef"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint \n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=openai_eval)\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What did the workers do after finishing their work?\",\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=\"They went for rest.\",\n",
        "    retrieval_context=[\"After a long day at work, the workers went to the their beds to rest for the night\"]\n",
        ")\n",
        "result = evaluate([test_case], [answer_relevancy_metric])\n",
        "pprint(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOL_2R_9M84d"
      },
      "source": [
        "Discussion: What is pretty cool here? ðŸ‘€\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAngzQeiO6qG"
      },
      "source": [
        "Let's see more cool things...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_wkyc_Wst2"
      },
      "source": [
        "## Buzz word of the GenAI era - Hallucinations.\n",
        "\n",
        "Can we test if model is hallucinating using Evals?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "d73e82d7604a48dbab006927cae7886f",
            "5dd9d4138cd94d348e4963ce2f17216c",
            "775c703086f64bdf867636b6bd0f1644",
            "551c9d208315465a9d35b501969b08f0"
          ]
        },
        "id": "-Q6yhBjzWJ_7",
        "outputId": "d23b0b3e-b970-4fe1-fb23-1ce215a03923"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Replace this with the actual documents that you are passing as input to your LLM.\n",
        "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
        "\n",
        "# Replace this with the actual output from your LLM application\n",
        "actual_output=\"A blond was drinking in public.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What was the blond doing?\",\n",
        "    actual_output=actual_output,\n",
        "    context=context\n",
        ")\n",
        "metric = HallucinationMetric(threshold=0.5, model= openai_eval)\n",
        "\n",
        "metric.measure(test_case)\n",
        "pprint(metric.score)\n",
        "pprint(metric.reason)\n",
        "\n",
        "# or evaluate test cases in bulk\n",
        "evaluate([test_case], [metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "7461bca76b5f48c5b869d52a57a47316",
            "cf7a7d576ed94348894808c06101ba39",
            "dfd1d092907f4cc3af2f40b5ffbeed60",
            "2dc8902eb34c48cd88176196c7e71340"
          ]
        },
        "id": "6ybjmu-OV4lE",
        "outputId": "90bd5613-17e3-443d-80b3-7f29505ae1ad"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Replace this with the actual documents that you are passing as input to your LLM.\n",
        "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
        "\n",
        "# Replace this with the actual output from your LLM application\n",
        "actual_output=\"A blond drinking water in public.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What was the blond doing?\",\n",
        "    actual_output=actual_output,\n",
        "    context=context\n",
        ")\n",
        "metric = HallucinationMetric(threshold=0.5, model= openai_eval)\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)\n",
        "\n",
        "# or evaluate test cases in bulk\n",
        "evaluate([test_case], [metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's go one level deep and measure TOXICITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepeval.metrics import ToxicityMetric\n",
        "\n",
        "# Replace this with the actual documents that you are passing as input to your LLM.\n",
        "input_content=\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"\n",
        "\n",
        "# Replace this with the actual output from your LLM application\n",
        "actual_output=\"A blond drinking water in public.\"\n",
        "\n",
        "toxicity_metric = ToxicityMetric(threshold=0.9, model=openai_eval)\n",
        "test_case = LLMTestCase(\n",
        "    input=input_content,\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=actual_output\n",
        ")\n",
        "\n",
        "toxicity_metric.measure(test_case)\n",
        "print(toxicity_metric.score)\n",
        "print(toxicity_metric.reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Should always remember to be INCLUSIVE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
        "\n",
        "# for details on how to create more such metrics, refer - https://docs.confident-ai.com/docs/metrics-llm-evals\n",
        "inclusivity_metric = GEval(\n",
        "    name=\"Inclusivity\",\n",
        "    criteria=\"Determine whether the output uses only inclusive language based on the expected output.\",\n",
        "    evaluation_steps=[\n",
        "        \"Check whether there are any terms, phrases, or structures that could be considered exclusive, biased, or marginalizing based on gender, race, ethnicity, ability, age, or other identity factors.'\",\n",
        "        \"If any such terms are found, the output is considered not inclusive.\",\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        "    model = openai_eval,\n",
        "    threshold = 0.9,\n",
        "    strict_mode = True\n",
        "    # verbose_mode = True\n",
        ")\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    # Input to LLM\n",
        "    input=\"Create a course for tech leaders on how to write application for tech accelorator programs.\",\n",
        "    actual_output=\"Each applicant should ensure that he finishes the application before the deadline\",\n",
        "\n",
        ")\n",
        "\n",
        "inclusivity_metric.measure(test_case)\n",
        "print(f\"Inclusivity score: {inclusivity_metric.score}\")\n",
        "print(f\"Reason : {inclusivity_metric.reason}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "roleready",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15e1b33f415f4b5b81d5e199a02c30d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22b9cc92bff84586970d7268ac602163": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc8902eb34c48cd88176196c7e71340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9caa9bbd9648acad0d7a4b27bc135e": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_22b9cc92bff84586970d7268ac602163",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, asyncâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, asyncâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "551c9d208315465a9d35b501969b08f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd9d4138cd94d348e4963ce2f17216c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7461bca76b5f48c5b869d52a57a47316": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cf7a7d576ed94348894808c06101ba39",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, async_moâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, async_moâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "775c703086f64bdf867636b6bd0f1644": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_551c9d208315465a9d35b501969b08f0",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, async_moâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, async_moâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "cf7a7d576ed94348894808c06101ba39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73e82d7604a48dbab006927cae7886f": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5dd9d4138cd94d348e4963ce2f17216c",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, async_moâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, async_moâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "dfd1d092907f4cc3af2f40b5ffbeed60": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2dc8902eb34c48cd88176196c7e71340",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, async_moâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, async_moâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "f64a606acc9b45d3b9cdd35b9ad9b043": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_15e1b33f415f4b5b81d5e199a02c30d0",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Custom Azure OpenAI Model, strict=False, asyncâ€¦</span>\n</pre>\n",
                  "text/plain": "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Custom Azure OpenAI Model, strict=False, asyncâ€¦\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
