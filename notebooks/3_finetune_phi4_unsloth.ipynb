{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f32f99",
   "metadata": {},
   "source": [
    "# Fine-Tuning Phi-4 with PEFT (LoRA)\n",
    "\n",
    "This notebook shows how to fine-tune the `OpenAI/phi-4` model from Hugging Face using PEFT with LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370a7a3",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fca000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "# !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "# !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0e9a",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abf2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel  # FastVisionModel for LLMs\n",
    "import torch\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
    "    \"unsloth/Phi-4\",  # Phi-4 2x faster!\n",
    "    \"unsloth/Phi-4-unsloth-bnb-4bit\",  # Phi-4 Unsloth Dynamic 4-bit Quant\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",  # Gemma 2x faster!\n",
    "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Qwen 2.5 2x faster!\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "]  # More models at https://docs.unsloth.ai/get-started/all-our-models\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-4\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ab53c",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Dataset\n",
    "Using the Alpaca-cleaned instruction dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize = False, add_generation_prompt = False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dec10",
   "metadata": {},
   "source": [
    "## 4. Preprocess & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b05718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a67a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1b5d5",
   "metadata": {},
   "source": [
    "## 5. Prepare Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7045d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118e425",
   "metadata": {},
   "source": [
    "## 6. Data Collator & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e97b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user<|im_sep|>\",\n",
    "    response_part=\"<|im_start|>assistant<|im_sep|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59836253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f2416",
   "metadata": {},
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c445ee9",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, max_new_tokens = 64, use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ccd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34170130",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "- Tune LoRA and training hyperparameters.\n",
    "- Replace with your own dataset.\n",
    "- Try full fine-tuning by disabling LoRA adapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roleready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
