{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70920ca2",
   "metadata": {},
   "source": [
    "# Simple RAG Application with Chroma VectorDB\n",
    "\n",
    "This notebook demonstrates a basic Retrieval-Augmented Generation (RAG) setup using Chroma as the vector database and OpenAI's embeddings & LLM for querying.\n",
    "\n",
    "Steps:\n",
    "1. Install dependencies\n",
    "2. Load documents\n",
    "3. Create embeddings and store in Chroma\n",
    "4. Perform retrieval and generate answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8487d",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c44433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb langchain-community openai tiktoken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae9cf3",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url='https://api.openai.com/v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304f2b3",
   "metadata": {},
   "source": [
    "## 3. Load Documents\n",
    "Replace `'docs/'` with your folder containing text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "doc_paths = glob.glob('../docs/*.txt')\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    docs.append({'text': content, 'metadata': {'source': os.path.basename(path)}})\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe072a",
   "metadata": {},
   "source": [
    "## 4. Create Embeddings and Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "txt_emb = OpenAIEmbeddings()\n",
    "\n",
    "# Create Chroma vector store\n",
    "vectordb = Chroma.from_texts(\n",
    "    [d['text'] for d in docs],\n",
    "    embedding=txt_emb,\n",
    "    metadatas=[d['metadata'] for d in docs],\n",
    "    persist_directory='chroma_db'\n",
    ")\n",
    "\n",
    "# Persist to disk\n",
    "vectordb.persist()\n",
    "print('Chroma vector store created and persisted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc1ab8",
   "metadata": {},
   "source": [
    "## 5. Retrieval-Augmented Generation\n",
    "Use the vector store to retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Tell me more about generative AI\")\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5d24d",
   "metadata": {},
   "source": [
    "### Use the vector store to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fa7c7",
   "metadata": {},
   "source": [
    "### Important information\n",
    "\n",
    "If you look at [docs/doc1.txt](../docs/doc1.txt), I have added this extra line that `Generative AI for discovered by Dr. Anush Sankaran.`\n",
    "\n",
    "There is no chance that the LLM would have known this secret (!) by default. However, when you ask the question `Tell me more about generative AI?`, you could see that the answer would include information about `Dr. Anush Sankaran`. \n",
    "\n",
    "This demonstrates that LLM retrieves relevant document information from the vectorDB, adds this information to the context, while generating an answer - which is the whole concept of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfa4d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generative AI is a type of algorithm that allows machines to generate new content by learning from existing data. It has various applications, such as content creation and code generation. It was discovered by Dr. Anush Sankaran."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | client\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"Tell me more about generative AI\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee2162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roleready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
