{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2613b33",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning Practice Notebook\n",
    "\n",
    "This exercise notebook leaves gaps in the code (`# TODO`) for you to implement key steps in fine-tuning a language model with PEFT (LoRA).\n",
    "Follow the steps and fill in the missing code to complete the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25300b22",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "```bash\n",
    "!pip install transformers datasets accelerate peft bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdca1c4",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53461272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2\"\n",
    "OUTPUT_DIR = \"./finetuned_gpt2\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa90fbd",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset `wikitext-2-raw-v1` into a `dataset` variable\n",
    "# dataset = load_dataset(...)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfba55",
   "metadata": {},
   "source": [
    "## 4. Preprocess & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(...)\n",
    "\n",
    "# Ensure pad_token\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    # TODO: Tokenize examples['text'] with padding and truncation\n",
    "    # inputs = tokenizer(...)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "    return inputs\n",
    "\n",
    "# TODO: Apply preprocessing to dataset splits, removing original text column\n",
    "# tokenized_dataset = dataset.map(...)\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc242d2",
   "metadata": {},
   "source": [
    "## 5. Prepare Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load base model for causal LM\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(..., load_in_8bit=True, device_map='auto')\n",
    "\n",
    "# TODO: Configure LoRA adapters\n",
    "# peft_config = LoraConfig(...)\n",
    "\n",
    "# TODO: Wrap model with LoRA\n",
    "# model = get_peft_model(base_model, peft_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa649e5",
   "metadata": {},
   "source": [
    "## 6. Data Collator & Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a DataCollatorForLanguageModeling instance\n",
    "# data_collator = DataCollatorForLanguageModeling(...)\n",
    "\n",
    "# TODO: Define TrainingArguments\n",
    "# training_args = TrainingArguments(...)\n",
    "\n",
    "# TODO: Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset['train'],\n",
    "#     eval_dataset=tokenized_dataset.get('validation'),\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cdcdb",
   "metadata": {},
   "source": [
    "## 7. Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Launch the training\n",
    "# trainer.train()\n",
    "\n",
    "# TODO: Save the fine-tuned model and tokenizer\n",
    "# model.save_pretrained(OUTPUT_DIR)\n",
    "# tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81716542",
   "metadata": {},
   "source": [
    "## 8. Inference\n",
    "After training, test your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the pipeline API or model.generate to produce text\n",
    "# from transformers import pipeline\n",
    "# generator = pipeline(...)\n",
    "# print(generator(\"Once upon a time\", max_length=50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829511ee",
   "metadata": {},
   "source": [
    "**Good luck!** Complete the TODOs to gain hands-on experience with LLM fine-tuning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
