{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf8bc74",
   "metadata": {},
   "source": [
    "# RAG Exercise: Building a Simple RAG Pipeline\n",
    "\n",
    "Complete the missing parts (`# TODO`) to build a basic Retrieval-Augmented Generation pipeline using Chroma and OpenAI embeddings + LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948909cc",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install dependencies and import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -U chromadb langchain openai\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url='https://api.openai.com/v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70250133",
   "metadata": {},
   "source": [
    "## 2. Load Documents\n",
    "Read text files from a folder into a `docs` list of dicts with `text` and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adjust the path to your docs directory if needed\n",
    "doc_paths = glob.glob('../docs/*.txt')  \n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    docs.append({\n",
    "        'text': content,\n",
    "        'metadata': {'source': os.path.basename(path)}\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92728bc",
   "metadata": {},
   "source": [
    "## 3. Embedding & Vector Store Ingestion\n",
    "Create embeddings and ingest documents into Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb751960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "# TODO: instantiate OpenAIEmbeddings with your API key if needed\n",
    "txt_emb = OpenAIEmbeddings()  \n",
    "\n",
    "# TODO: Create a Chroma vector store from texts\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=[d['text'] for d in docs],           # your document texts\n",
    "    embedding=txt_emb,                          # embedding function\n",
    "    metadatas=[d['metadata'] for d in docs],    # metadata list\n",
    "    persist_directory='chroma_db'               # where to persist\n",
    ")\n",
    "\n",
    "# Persist to disk\n",
    "vectordb.persist()\n",
    "print(\"Chroma vector store created and persisted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478a5be",
   "metadata": {},
   "source": [
    "## 4. Retrieval Function\n",
    "Implement a function to retrieve top-k similar documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621219bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query: str, k: int = 3):\n",
    "    \"\"\"Retrieve top-k documents for the query.\"\"\"\n",
    "    # TODO: Use vectordb.similarity_search or as_retriever to get documents\n",
    "    # Example using similarity_search:\n",
    "    # results = vectordb.similarity_search(query, k=k)\n",
    "    # return [doc.page_content for doc in results]\n",
    "    pass\n",
    "\n",
    "# Test retrieval\n",
    "sample_query = \"Tell me more about generative AI\"\n",
    "retrieved = retrieve_docs(sample_query, k=2)\n",
    "print(\"Retrieved Documents:\", retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df35b6",
   "metadata": {},
   "source": [
    "## 5. Build RetrievalQA Chain\n",
    "Initialize the LLM and RetrievalQA, then run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your own prompt for question answering using RAG\n",
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | client\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"Tell me more about generative AI\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5911b8",
   "metadata": {},
   "source": [
    "## 6. Experiment\n",
    "- Change `k` in `retrieve_docs` to 1 or 5 and observe differences.\n",
    "- Try different `chain_type` options: 'stuff', 'map_reduce', 'refine'.\n",
    "- Try different `prompts` in the `rag_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0a121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roleready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
